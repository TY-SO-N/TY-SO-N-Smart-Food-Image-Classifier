{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ImAP9rJGw50x"
   },
   "outputs": [],
   "source": [
    "# !nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z_piFRhPyUOX",
    "outputId": "5d982954-0f7d-4c92-e29b-5652c9750df1"
   },
   "outputs": [],
   "source": [
    "# Get helper functions file\n",
    "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LTyn07p0yt9l"
   },
   "outputs": [],
   "source": [
    "# Import series of helper functions for the notebook (we've created/used these in previous notebooks)\n",
    "from helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, compare_historys, walk_through_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X2i9udvtywcG"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def plot_loss_curves(history):\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "    epochs = range(len(loss))\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, loss, label='Training Loss')\n",
    "    plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, accuracy, label='Training Accuracy')\n",
    "    plt.plot(epochs, val_accuracy, label='Validation Accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def load_and_prep_image(filename, img_shape=224):\n",
    "    img = tf.io.read_file(filename)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.resize(img, size=[img_shape, img_shape])\n",
    "    img = img / 255.\n",
    "    return img\n",
    "\n",
    "def pred_and_plot(model, filename, class_names):\n",
    "    img = load_and_prep_image(filename)\n",
    "    pred = model.predict(tf.expand_dims(img, axis=0))\n",
    "    pred_class_index = pred.argmax()\n",
    "\n",
    "    if 0 <= pred_class_index < len(class_names):\n",
    "        pred_class = list(class_names.keys())[list(class_names.values()).index(pred_class_index)]\n",
    "    else:\n",
    "        pred_class = \"Unknown\"\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Prediction: {pred_class}\")\n",
    "    plt.axis(False)\n",
    "\n",
    "def create_tensorboard_callback(dir_name, experiment_name):\n",
    "    log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "    print(f\"Saving TensorBoard log files to: {log_dir}\")\n",
    "    return tensorboard_callback\n",
    "\n",
    "def create_base_model(input_shape=(224, 224, 3), output_shape=10, learning_rate=0.001, training=False):\n",
    "    base_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(include_top=False)\n",
    "    base_model.trainable = training\n",
    "\n",
    "    inputs = layers.Input(shape=input_shape, name=\"input_layer\")\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n",
    "    outputs = layers.Dense(units=output_shape, activation=\"softmax\", name=\"output_layer\")(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def compare_historys(original_history, new_history, initial_epochs=5):\n",
    "    acc = original_history.history[\"accuracy\"]\n",
    "    loss = original_history.history[\"loss\"]\n",
    "    val_acc = original_history.history[\"val_accuracy\"]\n",
    "    val_loss = original_history.history[\"val_loss\"]\n",
    "\n",
    "    total_acc = acc + new_history.history[\"accuracy\"]\n",
    "    total_loss = loss + new_history.history[\"loss\"]\n",
    "    total_val_acc = val_acc + new_history.history[\"val_accuracy\"]\n",
    "    total_val_loss = val_loss + new_history.history[\"val_loss\"]\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(total_acc, label='Training Accuracy')\n",
    "    plt.plot(total_val_acc, label='Validation Accuracy')\n",
    "    plt.plot([initial_epochs-1, initial_epochs-1], plt.ylim(), label='Start Fine Tuning')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(total_loss, label='Training Loss')\n",
    "    plt.plot(total_val_loss, label='Validation Loss')\n",
    "    plt.plot([initial_epochs-1, initial_epochs-1], plt.ylim(), label='Start Fine Tuning')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Q46l8F10afu"
   },
   "source": [
    "###Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3zVfhWGbyyUr"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sAjDE1Sl0KUH",
    "outputId": "f9a32a1f-08ca-45f1-db73-60da4733ad8e"
   },
   "outputs": [],
   "source": [
    "# Get all available datasets in TFDS\n",
    "datasets_list = tfds.list_builders()\n",
    "\n",
    "# Set our target dataset and see if it exists\n",
    "target_dataset = \"food101\"\n",
    "print(f\"'{target_dataset}' in TensorFlow Datasets: {target_dataset in datasets_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165,
     "referenced_widgets": [
      "306c899202094c6ca424b45efadf6ea0",
      "8f95bb27634b4385acc43a298cc63191",
      "eb4abcaca87841c99279db91d06d4ea6",
      "52217dfb25474887982cf48e01596918",
      "722c529f7f4843bbbfbb34c525f6f8fe",
      "18449fb43e4c4cbc9c7c01c8c61f8bcf",
      "5bbc5426b05d43e8b55c73c2af3b98a0",
      "74b6f4ab956f4170909ddb7340f3d419",
      "19ec40ec2c6f4fee80b079737b880c39",
      "e13c0b70bf044b0a8c0df5dd95f8cd96",
      "ca4da3532e2f4039bbdd5257bd943ac5",
      "56e0e01f3787468c9cf2427da40ca201",
      "d7e38f5cb06f4801a7788f36690e0270",
      "4803278d5fd64f009604da91967cfa99",
      "217a3a9ca6fb44b6b8dedb72f4cf3f9d",
      "d2c00d131ef3478f97d30aa6f45cb93e",
      "01a44844b48e4857af57a6c1937cbd76",
      "f7a79d6bb45d4785a6323b49c563581e",
      "17e37ca8bb4b4cecae8d3b05392ef341",
      "83c3072f9cbe42d48f4c3d6a574b5290",
      "42f595c1407d4732ada501e6688982d1",
      "ebf65854a7f14044811d5ee34a7fa324",
      "60c14c9d6fd74a61931dba49184514fa",
      "b8a32d4e08454447a484dfa6e1870e20",
      "080f070c133b432098251e3c7288b61c",
      "d01c1e5ec31647d588e941a2ca430fe1",
      "c19d90401d334621b578047466304023",
      "a383e37572164e41bc6343d66d101be9",
      "c976462febaa4bbe8fef9e6ea5a4b7cc",
      "fc7193cac5644f86ad6922cf0878ed10",
      "d78e4b04da944353ad5f9109868e0789",
      "20f43b8cc31846ba84ab6e351a077605",
      "c7e75db3bbee4228ba404e61fc8bda94",
      "cc5ccfc397f24b75b60914d2c63f62e1",
      "2927dd7703fc466eabf850c15303d652",
      "2e278c9378834a2ca3468d2e77b28d6f",
      "378929ce70164ac5a251f17fac460017",
      "9df011532cd447e0a524cbeb5d92ce7b",
      "ae72d7152da245828b317a1c1cdbda00",
      "a2e1283226174fa4be73c7ce7a99af4d",
      "5d3fe88bb8b2453f9ecf746e6f6c8c9c",
      "4befc11f0c764c7098e4f85cf335a41c",
      "3938dcac1728438f80af4e2db5ccf4b4",
      "68eeddad8dd04ef6a1fa4a74cd1ce849",
      "a158010aa6a840d99a917afb2554ac72",
      "4c4a74af24a240b5a650f621df8b82ae",
      "7496e267409f46c38271823100c0baa9",
      "4b93e735c6be49c8b3d5e5216bb94b2c",
      "6b4b8d926f9d432eb9daf0bdce91780a",
      "c451e88c98cc43f2803cc86ffae55e50",
      "d7774a815bc24722907149f0b1602517",
      "f42c2051268c47bfbacc3d5fdec32086",
      "c3cc97fc47c54934af5a1a4ae76326dc",
      "5ae2395f1a0f45788b80a45414d10783",
      "37dd280b2aed4b6399125f3161677f4b",
      "204e111e6c9f400b966dcd2e43d62192",
      "fb1d82c905634f338c0e45bef33034c7",
      "1c1af5bc7514464ba46032ce88ac1262",
      "fac8939d807a488782dec1bf9fc6ab60",
      "57149fb5f0164da0b7273fce2e501e27",
      "68753fccde8840bd9a85350d6e8be775",
      "8e152d0f6f3c42d5a0d1369848834065",
      "c5be083d6bb1430794a96ee06f7762dd",
      "14ecddfb856c4150bc05053cf2f636f6",
      "c28cd2b4831d4f3284ce1b8b5198b27d",
      "7617e15ef2c7475d941ed3d479e4b231",
      "fd69c2e7722545df944fc33d2adeac4f",
      "66732d285aa4483d967f2130308062c4",
      "dd3129c6365c4f308e410aed1938c8c3",
      "f5707ab7f6644e0ba33b37b6ddfff6a2",
      "29512e342ad7423989467ddce5c416c6",
      "acb566faaebe440aa0ca824cda1d31de",
      "3e1561fc3d0944209a0aa9e50fcc8686",
      "e60549a701934e7a8118be74b4b3a63f",
      "04910a5cd8c74c0baffa6874ebedca48",
      "722ad314203c4a33b31d774c2ef6adb8",
      "d2571861c2534775ab96533ef7608c67",
      "3feb77f3931d47478c0cdebe34cdef99",
      "db57d9819ebf4d75acae0f3b85d91f41",
      "6583c461a5c74f39ac967980328233b7",
      "5223538a300c46db8f5fcbfc79bd896a",
      "f0175d5ea27b499ab04fd1ae0789cae8",
      "a3b21fe795144dd5ba0b2c6020ba6e22",
      "0c52bbe611a742ee842ff9a3562b9ec9",
      "4de8ff9a19994c23afec026f703de44e",
      "f0af240079d443f5a3017f473d820021",
      "86c4714ae448418e896df539b6e26773",
      "d7405812ac554df0a7230e4f50ff0c9c"
     ]
    },
    "id": "ERoL2RPu0U5l",
    "outputId": "622e8c4b-7f28-43e4-a208-6f8a66cbdf42"
   },
   "outputs": [],
   "source": [
    "#Load data\n",
    "(train_data,test_data),ds_info = tfds.load(name=target_dataset,split=[\"train\",\"validation\"],as_supervised=True,with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NBXTMlz46Ysu",
    "outputId": "7c42a36b-379e-44a7-e7df-473024556348"
   },
   "outputs": [],
   "source": [
    "ds_info.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fPkydSqj6hav",
    "outputId": "545dd206-540f-4bd0-9a28-1a90a71d97e5"
   },
   "outputs": [],
   "source": [
    "class_names = ds_info.features[\"label\"].names\n",
    "class_names[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JNx686SA639U",
    "outputId": "c2448be2-d457-41d8-f7c2-e82cd24ec0a8"
   },
   "outputs": [],
   "source": [
    "class_names[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nffIOf0b7CE5"
   },
   "outputs": [],
   "source": [
    "# Take one sample off the training data\n",
    "train_one_sample = train_data.take(1) # samples are in format (image_tensor, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0lulT-Gf8hr4",
    "outputId": "a39e2e92-353e-4c09-a77c-10343082687c"
   },
   "outputs": [],
   "source": [
    "# Output info about our training sample\n",
    "for image, label in train_one_sample:\n",
    "  print(f\"\"\"\n",
    "  Image shape: {image.shape}\n",
    "  Image dtype: {image.dtype}\n",
    "  Target class from Food101 (tensor form): {label}\n",
    "  Class name (str form): {class_names[label.numpy()]}\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PGovaP8R8kkk",
    "outputId": "2c7b351a-6d91-4321-8772-29c61ec34d19"
   },
   "outputs": [],
   "source": [
    "# What does an image tensor from TFDS's Food101 look like?\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MqNG2oi_8nvv"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wFUlIFJ59NaY",
    "outputId": "a92bcc2b-b221-4d1f-aa4d-40e2a721ac84"
   },
   "outputs": [],
   "source": [
    "tf.reduce_min(image),tf.reduce_max(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "81wfW38B9SUa",
    "outputId": "deb98b87-797d-4f05-8af0-8ab23eb04f2d"
   },
   "outputs": [],
   "source": [
    "plt.imshow(image)\n",
    "plt.title(class_names[label.numpy()])\n",
    "plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kMlYuZk9-Mi_"
   },
   "outputs": [],
   "source": [
    "def show_random_sample(dataset, class_names):\n",
    "    sample = dataset.shuffle(1000).take(1)\n",
    "    for image, label in sample:\n",
    "        plt.imshow(image.numpy())\n",
    "        plt.title(class_names[label.numpy()])\n",
    "        plt.axis(False)\n",
    "        print(f\"Class label: {label.numpy()} ({class_names[label.numpy()]})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GGZyQPva_kHb"
   },
   "outputs": [],
   "source": [
    "show_random_sample(train_data, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toTeQw4B_lTy"
   },
   "source": [
    "###Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M4yjWv4F__3D"
   },
   "outputs": [],
   "source": [
    "# Make a function for preprocessing images\n",
    "def preprocess_img(image, label, img_shape=224):\n",
    "    \"\"\"\n",
    "    Converts image datatype from 'uint8' -> 'float32' and reshapes image to\n",
    "    [img_shape, img_shape, color_channels]\n",
    "    \"\"\"\n",
    "    image = tf.image.resize(image, [img_shape, img_shape]) # reshape to img_shape\n",
    "    return tf.cast(image, tf.float32), label # return (float32_image, label) tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2yiYHJqKA66i",
    "outputId": "ef4cdf6c-f82d-470e-9500-8fba32cb2b2b"
   },
   "outputs": [],
   "source": [
    "# Preprocess a single sample image and check the outputs\n",
    "preprocessed_img = preprocess_img(image, label)[0]\n",
    "print(f\"Image before preprocessing:\\n {image[:2]}...,\\nShape: {image.shape},\\nDatatype: {image.dtype}\\n\")\n",
    "print(f\"Image after preprocessing:\\n {preprocessed_img[:2]}...,\\nShape: {preprocessed_img.shape},\\nDatatype: {preprocessed_img.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "XCS4cSggA_9-",
    "outputId": "77cc5b03-8d21-47a0-fb00-f26d2b43121a"
   },
   "outputs": [],
   "source": [
    "# We can still plot our preprocessed image as long as we\n",
    "# divide by 255 (for matplotlib capatibility)\n",
    "plt.imshow(preprocessed_img/255.)\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SdIB-8AgCi-R"
   },
   "outputs": [],
   "source": [
    "# Map preprocessing function to training data (and paralellize)\n",
    "train_data = train_data.map(map_func=preprocess_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# Shuffle train_data and turn it into batches and prefetch it (load it faster)\n",
    "train_data = train_data.shuffle(buffer_size=1000).batch(batch_size=32).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Map prepreprocessing function to test data\n",
    "test_data = test_data.map(preprocess_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# Turn test data into batches (don't need to shuffle)\n",
    "test_data = test_data.batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvfd_W6EG87f"
   },
   "outputs": [],
   "source": [
    "# train_data = (train_data\n",
    "#               .unbatch()                     # remove the accidental batch axis\n",
    "#               .map(preprocess_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "#               .shuffle(1000)\n",
    "#               .batch(32)                     # exactly one batch step\n",
    "#               .prefetch(tf.data.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VKKkJhWZKOah",
    "outputId": "50feca17-cc84-4bdf-8823-768fea25f2ca"
   },
   "outputs": [],
   "source": [
    "train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qw1GBLs7L26-",
    "outputId": "348fcc2c-5aed-47f6-a63f-c2f956479149"
   },
   "outputs": [],
   "source": [
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqyBlaenL3ds"
   },
   "outputs": [],
   "source": [
    "# Create TensorBoard callback (already have \"create_tensorboard_callback()\" from a previous notebook)\n",
    "from helper_functions import create_tensorboard_callback\n",
    "import tensorflow as tf # Ensure tensorflow is imported for callbacks\n",
    "\n",
    "# Create ModelCheckpoint callback to save model's progress\n",
    "# saving weights requires \".weights.h5\" extension when save_weights_only=True\n",
    "checkpoint_path = \"model_checkpoints/cp.weights.h5\"\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                                      monitor=\"val_accuracy\", # save the model weights with best validation accuracy\n",
    "                                                      save_best_only=True, # only save the best weights\n",
    "                                                      save_weights_only=True, # only save model weights (not whole model)\n",
    "                                                      verbose=0) # don't print out whether or not model is being saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNkDo8WSGdFa"
   },
   "source": [
    "##Mixed Precision Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kMSOMgQ1HGXH"
   },
   "outputs": [],
   "source": [
    "# Turn on mixed precision training\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy(policy=\"mixed_float16\") # set global policy to mixed precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vCrf8_diHaga",
    "outputId": "1a2c96d4-b867-4834-dd6e-ad8fed3e9663"
   },
   "outputs": [],
   "source": [
    "mixed_precision.global_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YO8PZNVvHdpT"
   },
   "source": [
    "##Feature extraction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NoiJ6EQQHnfY",
    "outputId": "ce4e93ec-ac2b-4263-d815-7e6ad0b5bd9e"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# Create base model\n",
    "input_shape = (224, 224, 3)\n",
    "base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n",
    "base_model.trainable = False # freeze base model layers\n",
    "\n",
    "# Create Functional model\n",
    "inputs = layers.Input(shape=input_shape, name=\"input_layer\")\n",
    "# Note: EfficientNetBX models have rescaling built-in but if your model didn't you could have a layer like below\n",
    "# x = layers.Rescaling(1./255)(x)\n",
    "x = base_model(inputs, training=False) # set base_model to inference mode only\n",
    "x = layers.GlobalAveragePooling2D(name=\"pooling_layer\")(x)\n",
    "x = layers.Dense(len(class_names))(x) # want one output neuron per class\n",
    "# Separate activation of output layer so we can output float32 activations\n",
    "outputs = layers.Activation(\"softmax\", dtype=tf.float32, name=\"softmax_float32\")(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", # Use sparse_categorical_crossentropy when labels are *not* one-hot\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "68wQ3xelH7_3",
    "outputId": "3733aff6-c488-4fff-a971-156da3ee625b"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OPpGcHoRI7By",
    "outputId": "74c154ed-c5c6-4ae0-c4c2-e3fda6574234"
   },
   "outputs": [],
   "source": [
    "# Turn off all warnings except for errors\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Fit the model with callbacks\n",
    "history_101_food_classes_feature_extract = model.fit(train_data,\n",
    "                                                     epochs=3,\n",
    "                                                     steps_per_epoch=len(train_data),\n",
    "                                                     validation_data=test_data,\n",
    "                                                     validation_steps=int(0.15 * len(test_data)),\n",
    "                                                     callbacks=[create_tensorboard_callback(\"training_logs\",\n",
    "                                                                                            \"efficientnetb0_101_classes_all_data_feature_extract\"),\n",
    "                                                                model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21kjmsUnJT2b",
    "outputId": "909673b3-3358-4338-cd37-dd18149b71fc"
   },
   "outputs": [],
   "source": [
    "# Evaluate model (unsaved version) on whole test dataset\n",
    "results_feature_extract_model = model.evaluate(test_data)\n",
    "results_feature_extract_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sCigvkhyJcfD",
    "outputId": "cc569764-e97e-4f0b-d3c2-4e91f13f4148"
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "  # Create base model\n",
    "  input_shape = (224, 224, 3)\n",
    "  base_model = tf.keras.applications.efficientnet.EfficientNetB0(include_top=False)\n",
    "  base_model.trainable = False # freeze base model layers\n",
    "\n",
    "  # Create Functional model\n",
    "  inputs = layers.Input(shape=input_shape, name=\"input_layer\")\n",
    "  # Note: EfficientNetBX models have rescaling built-in but if your model didn't you could have a layer like below\n",
    "  # x = layers.Rescaling(1./255)(x)\n",
    "  x = base_model(inputs, training=False) # set base_model to inference mode only\n",
    "  x = layers.GlobalAveragePooling2D(name=\"pooling_layer\")(x)\n",
    "  x = layers.Dense(len(class_names))(x) # want one output neuron per class\n",
    "  # Separate activation of output layer so we can output float32 activations\n",
    "  outputs = layers.Activation(\"softmax\", dtype=tf.float32, name=\"softmax_float32\")(x)\n",
    "  model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "  return model\n",
    "\n",
    "# 2. Create and compile a new version of the original model (new weights)\n",
    "created_model = create_model()\n",
    "created_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                      optimizer=tf.keras.optimizers.Adam(),\n",
    "                      metrics=[\"accuracy\"])\n",
    "\n",
    "# 3. Load the saved weights\n",
    "created_model.load_weights(checkpoint_path)\n",
    "\n",
    "# 4. Evaluate the model with loaded weights\n",
    "results_created_model_with_loaded_weights = created_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jsgVZu2tK1Mr"
   },
   "outputs": [],
   "source": [
    "# # 5. Loaded checkpoint weights should return very similar results to checkpoint weights prior to saving\n",
    "# import numpy as np\n",
    "# assert np.isclose(results_feature_extract_model, results_created_model_with_loaded_weights).all(), \"Loaded weights results are not close to original model.\"  # check if all elements in array are close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NwlIprDgK3zv"
   },
   "outputs": [],
   "source": [
    "# Setup EarlyStopping callback to stop training if model's val_loss doesn't improve for 3 epochs\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", # watch the val loss metric\n",
    "                                                  patience=3) # if val loss decreases for 3 epochs in a row, stop training\n",
    "\n",
    "# Create ModelCheckpoint callback to save best model during fine-tuning\n",
    "# Provide a filepath with a .keras extension when saving the entire model (save_best_only=True)\n",
    "checkpoint_path = \"fine_tune_checkpoints/best_model.keras\"\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                                      save_best_only=True,\n",
    "                                                      monitor=\"val_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZvMQW1NLrIj"
   },
   "outputs": [],
   "source": [
    "# Creating learning rate reduction callback\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n",
    "                                                 factor=0.2, # multiply the learning rate by 0.2 (reduce by 5x)\n",
    "                                                 patience=2,\n",
    "                                                 verbose=1, # print out when learning rate goes down\n",
    "                                                 min_lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RXXu2syHLzc9"
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", # sparse_categorical_crossentropy for labels that are *not* one-hot\n",
    "                        optimizer=tf.keras.optimizers.Adam(0.0001), # 10x lower learning rate than the default\n",
    "                        metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bTPTwL1oL1TQ"
   },
   "outputs": [],
   "source": [
    "# # Start to fine-tune (all layers)\n",
    "# history_101_food_classes_all_data_fine_tune = model.fit(train_data,\n",
    "#                                                         epochs=100, # fine-tune for a maximum of 100 epochs\n",
    "#                                                         steps_per_epoch=len(train_data),\n",
    "#                                                         validation_data=test_data,\n",
    "#                                                         validation_steps=int(0.15 * len(test_data)), # validation during training on 15% of test data\n",
    "#                                                         callbacks=[create_tensorboard_callback(\"training_logs\", \"efficientb0_101_classes_all_data_fine_tuning\"), # track the model training logs\n",
    "#                                                                    model_checkpoint, # save only the best model during training\n",
    "#                                                                    early_stopping, # stop model after X epochs of no improvements\n",
    "#                                                                    reduce_lr]) # reduce the learning rate after X epochs of no improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A4uiCC3uNV4O",
    "outputId": "b840b132-29a2-4499-cad8-90cf210dcd08"
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PHyA9gwPV_3h"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, mixed_precision\n",
    "# from tensorflow.keras.optimizers import Adam, AdamW\n",
    "# from tensorflow.keras.optimizers.schedules import CosineDecayRestarts\n",
    "# from tensorflow.keras.callbacks import (\n",
    "#     EarlyStopping,\n",
    "#     ModelCheckpoint,\n",
    "#     ReduceLROnPlateau\n",
    "# )\n",
    "# from google.colab import drive\n",
    "\n",
    "# # ── 0. YOUR DATA & CLASSES ────────────────────────────────────────────────────\n",
    "# # Define these before running:\n",
    "# #   train_data: tf.data.Dataset of (image, label)\n",
    "# #   test_data:  tf.data.Dataset of (image, label)\n",
    "# #   class_names: list of 101 food-category strings\n",
    "\n",
    "# # ── 1. MOUNT DRIVE & CREATE SAVE DIR ─────────────────────────────────────────\n",
    "# drive.mount('/content/drive', force_remount=True)\n",
    "# SAVE_DIR = '/content/drive/MyDrive/food_vision/07_efficientnetb0_fine_tuned_101_classes_mixed_precision'\n",
    "# os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "# os.makedirs('fine_tune_checkpoints', exist_ok=True)  # for intermediate ckpts\n",
    "\n",
    "# # ── 2. SPEED: MIXED PRECISION & XLA JIT ───────────────────────────────────────\n",
    "# mixed_precision.set_global_policy('mixed_float16')\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "\n",
    "# # ── 3. DATA PIPELINE: TF.DATA + AUGMENTATION ─────────────────────────────────\n",
    "# BATCH_SIZE = 64    # T4 has 16 GB VRAM; increase if you can\n",
    "# IMG_SIZE   = 224\n",
    "\n",
    "# augment_layers = tf.keras.Sequential([\n",
    "#     layers.RandomFlip('horizontal'),\n",
    "#     layers.RandomRotation(0.1),\n",
    "#     layers.RandomZoom(0.1),\n",
    "#     layers.RandomContrast(0.1),\n",
    "# ])\n",
    "\n",
    "# def preprocess(image, label):\n",
    "#     image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n",
    "#     image = tf.keras.applications.efficientnet.preprocess_input(image)\n",
    "#     return image, label\n",
    "\n",
    "# def make_ds(dataset, shuffle=False, augment=False):\n",
    "#     ds = dataset\n",
    "#     if shuffle:\n",
    "#         ds = ds.shuffle(2048)\n",
    "#     ds = ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "#     if augment:\n",
    "#         ds = ds.map(\n",
    "#             lambda x, y: (augment_layers(x, training=True), y),\n",
    "#             num_parallel_calls=tf.data.AUTOTUNE\n",
    "#         )\n",
    "#     ds = ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "#     return ds\n",
    "\n",
    "# train_ds = make_ds(train_data, shuffle=True,  augment=True)\n",
    "# val_ds   = make_ds(test_data,   shuffle=False, augment=False)\n",
    "\n",
    "# # ── 4. MODEL BUILDING ─────────────────────────────────────────────────────────\n",
    "# def create_model(num_classes):\n",
    "#     inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "#     base   = tf.keras.applications.EfficientNetB0(\n",
    "#         include_top=False,\n",
    "#         input_tensor=inputs,\n",
    "#         weights='imagenet'\n",
    "#     )\n",
    "#     x = layers.GlobalAveragePooling2D()(base.output)\n",
    "#     x = layers.Dropout(0.3)(x)\n",
    "#     x = layers.Dense(num_classes)(x)\n",
    "#     outputs = layers.Activation('softmax', dtype='float32')(x)\n",
    "#     return tf.keras.Model(inputs, outputs), base\n",
    "\n",
    "# model, backbone = create_model(len(class_names))\n",
    "\n",
    "# # ── 5. FEATURE-EXTRACTION: TRAIN ONLY THE HEAD ───────────────────────────────\n",
    "# backbone.trainable = False\n",
    "# opt_head = AdamW(learning_rate=1e-3, weight_decay=1e-5)\n",
    "\n",
    "# model.compile(\n",
    "#     loss=tf.keras.losses.SparseCategoricalCrossentropy(label_smoothing=0.1),\n",
    "#     optimizer=opt_head,\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "\n",
    "# model.fit(\n",
    "#     train_ds,\n",
    "#     validation_data=val_ds,\n",
    "#     epochs=5,\n",
    "#     callbacks=[\n",
    "#         EarlyStopping(\n",
    "#             monitor='val_loss',\n",
    "#             patience=2,\n",
    "#             min_delta=1e-3,\n",
    "#             restore_best_weights=True\n",
    "#         )\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # ── 6. FINE-TUNING: PROGRESSIVE UNFREEZING ────────────────────────────────────\n",
    "# for layer in backbone.layers[-30:]:\n",
    "#     layer.trainable = True\n",
    "\n",
    "# steps_per_epoch = len(train_data) // BATCH_SIZE\n",
    "# total_steps     = steps_per_epoch * 50\n",
    "\n",
    "# lr_schedule = CosineDecayRestarts(\n",
    "#     initial_learning_rate=1e-4,\n",
    "#     first_decay_steps=total_steps // 5\n",
    "# )\n",
    "\n",
    "# opt_ft = AdamW(learning_rate=lr_schedule, weight_decay=1e-6)\n",
    "\n",
    "# model.compile(\n",
    "#     loss=tf.keras.losses.SparseCategoricalCrossentropy(label_smoothing=0.1),\n",
    "#     optimizer=opt_ft,\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "\n",
    "# model.fit(\n",
    "#     train_ds,\n",
    "#     validation_data=val_ds,\n",
    "#     epochs=50,\n",
    "#     callbacks=[\n",
    "#         EarlyStopping(\n",
    "#             monitor='val_loss',\n",
    "#             patience=3,\n",
    "#             min_delta=1e-3,\n",
    "#             restore_best_weights=True\n",
    "#         ),\n",
    "#         ModelCheckpoint(\n",
    "#             filepath=os.path.join('fine_tune_checkpoints', 'best_model.keras'),\n",
    "#             save_best_only=True,\n",
    "#             monitor='val_loss',\n",
    "#             verbose=1\n",
    "#         ),\n",
    "#         ReduceLROnPlateau(\n",
    "#             monitor='val_loss',\n",
    "#             factor=0.2,\n",
    "#             patience=2,\n",
    "#             verbose=1,\n",
    "#             min_lr=1e-7\n",
    "#         )\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # ── 7. SAVE FINAL WEIGHTS ─────────────────────────────────────────────────────\n",
    "# model.save_weights(os.path.join(SAVE_DIR, 'efficientnetb0_101_finetuned.ckpt'))\n",
    "# print(f'Training complete, weights saved to: {SAVE_DIR}')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
